import streamlit as st
from PIL import Image
import numpy as np
import torch
import easyocr
from transformers import AutoTokenizer, AutoModelForCausalLM


# =========================
# 1. CONFIG STREAMLIT
# =========================
st.set_page_config(
    page_title="OCR + LLM Chatbot (Ti·∫øng Vi·ªát - Offline)",
    layout="wide"
)

st.title("üìÑ OCR + ü§ñ Chatbot LLM (Ti·∫øng Vi·ªát - Offline/Free)")
st.write("Upload ·∫£nh ‚Üí OCR ‚Üí h·ªèi AI d·ª±a tr√™n n·ªôi dung trong ·∫£nh.")


# =========================
# 2. LOAD OCR
# =========================
@st.cache_resource
def load_ocr():
    return easyocr.Reader(["vi", "en"], gpu=torch.cuda.is_available())

reader = load_ocr()


# =========================
# 3. LOAD LLM (Qwen2.5-1.5B)
# =========================
@st.cache_resource
def load_llm():
    model_name = "Qwen/Qwen2.5-1.5B-Instruct"

    dtype = torch.float16 if torch.cuda.is_available() else torch.float32

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=dtype,
        device_map="auto"  # GPU n·∫øu c√≥
    )

    return tokenizer, model

tokenizer, model = load_llm()


# =========================
# 4. SINH TR·∫¢ L·ªúI T·ª™ LLM
# =========================
def answer_llm(ocr_text: str, question: str):
    device = model.device

    system_prompt = (
        "B·∫°n l√† tr·ª£ l√Ω AI hi·ªÉu ti·∫øng Vi·ªát. "
        "Ch·ªâ d·ª±a v√†o vƒÉn b·∫£n OCR ƒë∆∞·ª£c cung c·∫•p, h√£y tr·∫£ l·ªùi ch√≠nh x√°c ‚Äì ng·∫Øn g·ªçn ‚Äì r√µ r√†ng."
    )

    prompt = f"""
<|system|>
{system_prompt}
</s>
<|user|>
VƒÉn b·∫£n OCR:

{ocr_text}

C√¢u h·ªèi: {question}
</s>
<|assistant|>
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False,
            temperature=0.2,
            pad_token_id=tokenizer.eos_token_id
        )

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return text.split("assistant", 1)[-1].strip()


# =========================
# 5. UI
# =========================

if "ocr_text" not in st.session_state:
    st.session_state.ocr_text = ""


# Upload ·∫£nh
st.subheader("1Ô∏è‚É£ Upload ·∫£nh ƒë·ªÉ OCR")
uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh...", type=["jpg", "jpeg", "png"])

if uploaded_file:
    img = Image.open(uploaded_file).convert("RGB")
    st.image(img, use_column_width=True)

    if st.button("üîç Ch·∫°y OCR"):
        with st.spinner("ƒêang ch·∫°y OCR..."):
            ocr_result = reader.readtext(np.array(img))
            txt = "\n".join([r[1] for r in ocr_result])
            st.session_state.ocr_text = txt

        st.success("Ho√†n t·∫•t OCR!")
        st.text_area("üìå K·∫øt qu·∫£ OCR:", txt, height=200)


# Chatbot
st.subheader("2Ô∏è‚É£ H·ªèi AI d·ª±a tr√™n vƒÉn b·∫£n OCR")

if not st.session_state.ocr_text:
    st.info("H√£y upload ·∫£nh v√† ch·∫°y OCR tr∆∞·ªõc.")
else:
    q = st.text_input("Nh·∫≠p c√¢u h·ªèi:")
    if st.button("ü§ñ Tr·∫£ l·ªùi"):
        with st.spinner("AI ƒëang suy nghƒ©..."):
            ans = answer_llm(st.session_state.ocr_text, q)

        st.markdown("### üí° Tr·∫£ l·ªùi:")
        st.write(ans)
